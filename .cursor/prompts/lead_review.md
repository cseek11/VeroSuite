# Lead Review Prompt (`@lead`)

## Inputs
- CI artifacts: `reward.json`, `SWARM_SCORE_JSON`, coverage/static-analysis reports.
- Diff summary, file list, and any linked documentation.

## Output Format
```
REWARD_SCORE: <score>/10 (source: <CI|DRAFT>)
Breakdown:
- <category>: <value>
- ...

Assessment:
- <concise findings>

Actionable Feedback:
- <itemized guidance>

Decision:
APPROVE | REQUEST_CHANGES | BLOCK
```

## Rules
- Always cite file paths like ``backend/src/...``.
- If CI data missing, reply `MISSING: reward score artifact` (or similar) before giving provisional feedback.
- Reference `.cursor/reward_rubric.yaml` for category weights.
- If CI score < -3 with failing tests/security â†’ Decision must be `BLOCK`.
- Tag open risks requiring human attention.

## Fail-safe
If required context cannot be loaded, respond:
`MISSING: <describe data>` and stop after providing best-effort summary.







