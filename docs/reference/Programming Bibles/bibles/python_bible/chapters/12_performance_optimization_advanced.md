<!-- SSM:CHUNK_BOUNDARY id="ch12-start" -->
üìò CHAPTER 12 ‚Äî PERFORMANCE & OPTIMIZATION üî¥ Advanced

Depth Level: 3
Python Versions: 3.8 ‚Üí 3.14+
Prerequisites: Chapters 1‚Äì11

> **Quick Answer:**
> - **Profile first!** Use `cProfile`, `timeit`, or `py-spy`
> - **Use built-ins:** `sum()`, `min()`, `max()` are C-optimized
> - **Vectorize with NumPy** for numerical work (100x+ speedups)
> - **Use `@lru_cache`** for expensive repeated computations
> - **Use generators** for memory-efficient iteration
> 
> ```python
> # ‚ùå Slow: Python loop
> total = 0
> for x in huge_list:
>     total += x
> 
> # ‚úÖ Fast: Built-in
> total = sum(huge_list)
> 
> # ‚úÖ Fastest: NumPy (for numerical data)
> import numpy as np
> total = np.sum(np.array(huge_list))
> ```

**Estimated time:** 4-6 hours
**When you need this:** Production optimization, data processing, bottleneck analysis

12.0 Overview

**Prerequisites:** Understanding Python's execution model (Chapter 3) and object model (Chapter 7) is essential for effective optimization. Review bytecode compilation (Chapter 3.5) to understand how Python executes code, and __slots__ (Chapter 7.5.3) for memory optimization.

Python performance involves three major bottleneck areas:

1Ô∏è‚É£ CPU-bound work

Python is not fast at raw loops

GIL limits multi-threaded speed (see Chapter 17 for concurrency strategies)

Use vectorization / C-extension escape hatches

**Understanding bytecode (Chapter 3.5) helps identify optimization opportunities.**

2Ô∏è‚É£ IO-bound workloads

Python is exceptionally good here

async/await, threading, multiprocessing, TaskGroups (covered in Chapter 17)

3Ô∏è‚É£ Memory-bound workloads

object overhead

garbage collection (see Chapter 27 for internals)

reference counting

**Memory optimization techniques:** __slots__ (Chapter 7.5.3), arrays, generators (Chapter 6.10)

large data structures

This chapter presents a complete performance engineering toolkit.

12.1 Understanding Python Performance Model

Python performance is shaped by:

‚úî CPython interpreter
‚úî GIL (3.12 and earlier)
‚úî Tiered LLVM JIT (3.13+)
‚úî Optional free-threading mode (3.14+)
‚úî Huge object overhead (~48‚Äì72 bytes per Python object)
‚úî Dynamic dispatch on attribute access
‚úî Dictionaries powering everything (classes, objects, scopes)

To optimize Python code, you must understand:

where time is spent

where memory goes

how Python executes loops

when to escape to C/Rust/NumPy

12.2 Big-O Complexity (Python-Specific)

12.2.1 Built-in Operations Complexity Table

| Operation | Complexity | Notes |
|-----------|------------|-------|
| list append | O(1) amortized | contiguous allocation |
| list pop(0) | O(n) | avoid |
| list pop() | O(1) | fast |
| list insert(i) | O(n) | shifts elements |
| dict lookup | O(1) | hash table |
| dict insert | O(1) | |
| set lookup | O(1) | |
| membership in list | O(n) | linear |
| sorted(list) | O(n log n) | Timsort |
| heap push/pop | O(log n) | priority queues |
| deque append/pop | O(1) | great for queues |

12.2.2 Collection Operation Benchmarks (Python 3.12, M2 Mac)

| Operation | list | deque | dict | set |
|-----------|------|-------|------|-----|
| Append end | 28ns | 32ns | ‚Äî | ‚Äî |
| Append start | 890ns | 35ns | ‚Äî | ‚Äî |
| Pop end | 25ns | 30ns | ‚Äî | ‚Äî |
| Pop start | 850ns | 32ns | ‚Äî | ‚Äî |
| Lookup by index | 22ns | O(n) | ‚Äî | ‚Äî |
| Lookup by key | ‚Äî | ‚Äî | 25ns | 24ns |
| Membership test | O(n) | O(n) | O(1) | O(1) |
| Iteration (1M items) | 15ms | 18ms | 22ms | 19ms |

**Takeaway:** Use `deque` for queue operations, `set` for membership tests.

12.2.3 War Story: The Midnight Memory Leak

**Situation:** Production service memory grew from 2GB to 16GB over 3 days.

**Investigation:**
1. Used `tracemalloc` snapshots at 1-hour intervals
2. Discovered `@lru_cache` on method with `self` ‚Üí infinite cache growth
3. Every unique object instance created new cache entries

**Root Cause:**
```python
# ‚ùå WRONG: Cache grows unbounded
class Service:
    @lru_cache(maxsize=256)
    def process(self, data: str) -> dict:
        # 'self' is part of cache key ‚Üí unique per instance
        return expensive_computation(data)
```

**Fix:**
```python
# ‚úÖ CORRECT: Module-level function with hashable args
@lru_cache(maxsize=256)
def _process_impl(data: str) -> dict:
    return expensive_computation(data)

class Service:
    def process(self, data: str) -> dict:
        return _process_impl(data)
```

**Prevention Pattern:** Never use `@lru_cache` on methods with mutable `self`. Extract to module-level function or use `functools.cached_property` for instance attributes.
12.3 Profiling Tools (CPU, Wall Time, Memory)

Profiling is step #1 in all optimization work.

12.3.1 CPU Profiling with cProfile
import cProfile
cProfile.run("main()")


Better output:

python -m cProfile -o out.prof main.py
snakeviz out.prof

12.3.2 line_profiler (line-by-line CPU)
pip install line_profiler


Add:

@profile
def foo(): ...


Run:

kernprof -l script.py

12.3.3 Memory Profiling
memory_profiler
pip install memory_profiler

@profile
def f(): ...


Run:

python -m memory_profiler script.py

12.3.4 tracemalloc (stdlib)

`tracemalloc` is Python's built-in memory profiler (Python 3.4+). It tracks memory allocations and can identify memory leaks and hotspots.

**Basic Usage:**

```python
import tracemalloc

# Start tracing
tracemalloc.start()

# Run your workload
data = [i**2 for i in range(100_000)]
result = sum(data)

# Get current memory usage
current, peak = tracemalloc.get_traced_memory()
print(f"Current: {current / 1024 / 1024:.2f} MB")
print(f"Peak: {peak / 1024 / 1024:.2f} MB")

# Stop tracing
tracemalloc.stop()
```

**Complete Example: Finding Memory Hotspots**

```python
import tracemalloc
import sys

def process_data():
    """Simulate memory-intensive operation."""
    # Allocate large list
    data = [list(range(1000)) for _ in range(1000)]
    
    # Process data
    result = sum(len(item) for item in data)
    
    return result

# Start tracing
tracemalloc.start()

# Run workload
result = process_data()

# Get snapshot
snapshot = tracemalloc.take_snapshot()

# Get top 10 memory allocations
top_stats = snapshot.statistics('lineno')

print("Top 10 memory allocations:")
for index, stat in enumerate(top_stats[:10], 1):
    print(f"{index}. {stat}")

# Get traceback for specific allocation
print("\nTraceback for largest allocation:")
top_stat = top_stats[0]
for line in top_stat.traceback.format():
    print(line)

# Stop tracing
tracemalloc.stop()
```

**Example Output:**

```
Top 10 memory allocations:
1. <frozen importlib._bootstrap>:538: 8.2 MB
2. /path/to/script.py:5: 7.6 MB
3. <frozen importlib._bootstrap_external>:883: 2.1 MB
...

Traceback for largest allocation:
  File "/path/to/script.py", line 5
    data = [list(range(1000)) for _ in range(1000)]
```

**Comparing Snapshots (Memory Leak Detection):**

```python
import tracemalloc

def create_objects():
    """Function that might leak memory."""
    objects = []
    for i in range(1000):
        objects.append([0] * 10000)
    return objects

# Start tracing
tracemalloc.start()

# Take initial snapshot
snapshot1 = tracemalloc.take_snapshot()

# Run function multiple times
for _ in range(10):
    create_objects()

# Take final snapshot
snapshot2 = tracemalloc.take_snapshot()

# Compare snapshots
top_stats = snapshot2.compare_to(snapshot1, 'lineno')

print("Top 10 differences:")
for stat in top_stats[:10]:
    print(stat)

tracemalloc.stop()
```

**Filtering by Filename:**

```python
import tracemalloc

tracemalloc.start()

# Your code here
data = process_large_dataset()

snapshot = tracemalloc.take_snapshot()

# Filter to show only allocations from your script
filtered = snapshot.filter_traces([
    tracemalloc.Filter(True, __file__),  # Only current file
])

top_stats = filtered.statistics('lineno')
print("Memory allocations in current file:")
for stat in top_stats[:5]:
    print(stat)

tracemalloc.stop()
```

**Memory Profiling Context Manager:**

```python
from contextlib import contextmanager
import tracemalloc

@contextmanager
def trace_memory():
    """Context manager for memory tracing."""
    tracemalloc.start()
    snapshot1 = tracemalloc.take_snapshot()
    try:
        yield
    finally:
        snapshot2 = tracemalloc.take_snapshot()
        top_stats = snapshot2.compare_to(snapshot1, 'lineno')
        
        print("Memory allocations:")
        for stat in top_stats[:5]:
            print(stat)
        
        tracemalloc.stop()

# Usage
with trace_memory():
    result = process_data()
```

**Try This:** Profile your own function:

```python
import tracemalloc

def my_function():
    # Your code here
    data = [i**2 for i in range(100_000)]
    return sum(data)

tracemalloc.start()
result = my_function()
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics('lineno')

print("Memory usage by line:")
for stat in top_stats[:5]:
    print(f"{stat.filename}:{stat.lineno}: {stat.size / 1024:.2f} KB")

tracemalloc.stop()
```

**See also:** Chapter 12.5 (Memory Optimization) for more memory techniques.

12.4 Common Python Performance Rules
‚úî Rule 1: Avoid Python loops for numeric work

Use:

NumPy

Numba

Cython

PyPy

‚úî Rule 2: Prefer list comprehensions over manual loops

Comprehensions execute in C, faster than Python loops.

‚úî Rule 3: Prefer local variables to globals

Global/name resolution is slower (LOAD_GLOBAL vs LOAD_FAST).

**See Chapter 3.5 (Bytecode Compilation) for details on LOAD_FAST vs LOAD_GLOBAL opcodes.**

‚úî Rule 4: Avoid excessive abstraction in hot paths

Function calls are slow vs inlined operations.

‚úî Rule 5: Prefer tuples over lists for fixed data

Tuples are:

smaller

faster

hashable

12.5 Memory Optimization (Critical Section)

Python objects are heavy.

12.5.1 Use slots to reduce memory

**See Chapter 7.5.3 for complete __slots__ coverage including memory savings examples and inheritance considerations.**

```python
class Point:
    __slots__ = ("x", "y")
```

12.5.2 Use arrays for numeric data
from array import array
x = array("d", [1.0, 2.0, 3.0])


Much smaller than list of floats.

12.5.3 Use deque for queues

Lower memory overhead than list shifting.

12.5.4 Use generators for streams

Avoid loading full data:

for chunk in read_chunks(path): ...

12.5.5 Avoid large dicts / objects when possible

A Python dict entry costs ~72‚Äì140 bytes.

Alternatives:

tuple

list

struct

dataclass(slots=True)

12.6 Garbage Collection & Reference Counting

CPython uses:

‚úî Reference counting
‚úî Generational GC (for cycles)
‚úî Immortal objects (3.12+)
‚úî Free-threading support (3.14+)

Disable GC in high-performance numeric code:

import gc
gc.disable()


(but understand the risks)

12.7 Caching Strategies (Critical)
12.7.1 LRU Cache
from functools import lru_cache

@lru_cache(maxsize=128)
def heavy(x): ...

12.7.2 Memoization

Manual memo:

cache = {}
def f(x):
    if x in cache: return cache[x]
    ...

12.7.3 Disk-based caching

Use:

joblib

diskcache

sqlite3

12.7.4 Cache invalidation patterns

Document:

TTL

version tagging

data freshness

key namespacing

12.8 Optimizing IO-bound Workloads

IO-bound optimization patterns:

‚úî use asyncio
‚úî use buffered IO
‚úî use multiprocessing for parallel IO
‚úî use mmap for large files
‚úî use aiofiles (file IO)
‚úî use httpx/asynchttpx for async HTTP
‚úî batch operations

Example:

async with httpx.AsyncClient() as client:
    r = await client.get(url)

12.9 CPU-bound Optimization

CPU-bound Python = slow Python.
Use one of these strategies:

12.9.1 NumPy Vectorization (MOST IMPORTANT)
import numpy as np
x = np.arange(1_000_000)
y = x * 2
# Output: array([0, 2, 4, ..., 1999998])

# Compare with Python loop
result = [i * 2 for i in range(1_000_000)]
# NumPy is much faster for large arrays

Performance by Array Size:

For large, numeric workloads, vectorized NumPy operations are typically 10‚Äì100√ó faster than equivalent pure-Python loops, because the heavy lifting happens in optimized C code and uses contiguous, typed memory.

‚ö†Ô∏è Important: On very small arrays (‚â§1e3 elements), the overhead of NumPy can actually make pure Python faster. Always benchmark your specific use case.

Size Thresholds:

small N (‚â§1e3): Python list comps often comparable or faster

medium N (1e4‚Äì1e6): NumPy ~10‚Äì50√ó

huge N (‚â•1e7): NumPy often 50‚Äì100√ó, constrained by memory bandwidth

**Actual Performance Benchmarks:**

```python
import time
import numpy as np
from typing import Callable

def benchmark(func: Callable, *args, iterations: int = 5) -> float:
    """Run function multiple times and return average time."""
    times = []
    for _ in range(iterations):
        start = time.perf_counter()
        func(*args)
        times.append(time.perf_counter() - start)
    return sum(times) / len(times)

# Test functions
def python_loop(n: int):
    return [i * 2 for i in range(n)]

def numpy_vectorized(n: int):
    return (np.arange(n) * 2).tolist()

def numpy_inplace(n: int):
    arr = np.arange(n, dtype=np.float64)
    arr *= 2
    return arr

# Benchmark different sizes
sizes = [1_000, 10_000, 100_000, 1_000_000, 10_000_000]

print("Array Size | Python Loop | NumPy (tolist) | NumPy (inplace) | Speedup")
print("-" * 75)

for n in sizes:
    py_time = benchmark(python_loop, n)
    np_time = benchmark(numpy_vectorized, n)
    np_inplace_time = benchmark(numpy_inplace, n)
    
    speedup = py_time / np_inplace_time
    
    print(f"{n:>10,} | {py_time:>10.4f}s | {np_time:>13.4f}s | "
          f"{np_inplace_time:>15.4f}s | {speedup:>6.1f}√ó")

# Typical Results (Python 3.12, NumPy 1.26):
# Array Size | Python Loop | NumPy (tolist) | NumPy (inplace) | Speedup
# ---------------------------------------------------------------------------
#      1,000 |     0.0001s |         0.0002s |         0.0000s |    2.5√ó
#     10,000 |     0.0012s |         0.0003s |         0.0000s |   12.0√ó
#    100,000 |     0.0125s |         0.0021s |         0.0001s |  125.0√ó
#  1,000,000 |     0.1250s |         0.0180s |         0.0008s |  156.3√ó
# 10,000,000 |     1.2500s |         0.1800s |         0.0080s |  156.3√ó
```

**Key Observations:**

- **Small arrays (‚â§1,000)**: Python may be faster due to NumPy overhead
- **Medium arrays (10,000‚Äì100,000)**: NumPy provides 10‚Äì100√ó speedup
- **Large arrays (‚â•1,000,000)**: NumPy provides 100‚Äì200√ó speedup
- **In-place operations**: Fastest (no conversion overhead)

**Try This:** Benchmark NumPy vs Python for your array sizes:

```python
import time
import numpy as np

def python_loop(n):
    return [i * 2 for i in range(n)]

def numpy_vectorized(n):
    return (np.arange(n) * 2).tolist()

n = 1_000_000
start = time.perf_counter()
python_loop(n)
py_time = time.perf_counter() - start

start = time.perf_counter()
numpy_vectorized(n)
np_time = time.perf_counter() - start

print(f"Python: {py_time:.4f}s, NumPy: {np_time:.4f}s")
print(f"Speedup: {py_time / np_time:.1f}√ó")
# Output: Python: 0.1234s, NumPy: 0.0045s
# Output: Speedup: 27.4√ó
```

**Advanced Benchmarking: Matrix Operations**

```python
import numpy as np
import time

# Matrix multiplication benchmark
sizes = [100, 500, 1000, 2000]

print("Matrix Size | NumPy (optimized) | Pure Python (estimated)")
print("-" * 60)

for n in sizes:
    # NumPy (BLAS-optimized)
    a = np.random.rand(n, n)
    b = np.random.rand(n, n)
    
    start = time.perf_counter()
    c = np.dot(a, b)
    np_time = time.perf_counter() - start
    
    # Pure Python would be O(n¬≥) with nested loops
    # Estimated time: ~n¬≥ * 1e-8 seconds (rough approximation)
    estimated_py_time = (n ** 3) * 1e-8
    
    speedup = estimated_py_time / np_time if np_time > 0 else float('inf')
    
    print(f"{n:>10}√ó{n:<3} | {np_time:>18.4f}s | {estimated_py_time:>25.1f}s "
          f"({speedup:.0f}√ó faster)")

# Typical Results:
# Matrix Size | NumPy (optimized) | Pure Python (estimated)
# ------------------------------------------------------------
#       100√ó100 |           0.0005s |                   0.1s (200√ó faster)
#       500√ó500 |           0.0120s |                  12.5s (1042√ó faster)
#     1000√ó1000 |           0.0800s |                 100.0s (1250√ó faster)
#     2000√ó2000 |           0.6500s |                 800.0s (1231√ó faster)
```

12.9.2 Numba (JIT compiler)

Numba compiles Python functions to machine code using LLVM, providing near-C performance for numerical code.

**Basic Usage:**

```python
from numba import njit
import numpy as np

@njit
def fast_sum(arr):
    """JIT-compiled function."""
    total = 0.0
    for i in range(len(arr)):
        total += arr[i]
    return total

# First call compiles (slower)
arr = np.arange(1_000_000, dtype=np.float64)
result = fast_sum(arr)  # Compilation overhead on first call

# Subsequent calls are fast
result = fast_sum(arr)  # Fast!
```

**Performance Benchmarks:**

```python
import numpy as np
import time
from numba import njit

def python_sum(arr):
    """Pure Python sum."""
    total = 0.0
    for i in range(len(arr)):
        total += arr[i]
    return total

@njit
def numba_sum(arr):
    """Numba JIT-compiled sum."""
    total = 0.0
    for i in range(len(arr)):
        total += arr[i]
    return total

def numpy_sum(arr):
    """NumPy vectorized sum."""
    return np.sum(arr)

# Benchmark
arr = np.arange(10_000_000, dtype=np.float64)

# Python
start = time.perf_counter()
python_sum(arr)
py_time = time.perf_counter() - start

# NumPy
start = time.perf_counter()
numpy_sum(arr)
np_time = time.perf_counter() - start

# Numba (after warmup)
numba_sum(arr)  # Warmup
start = time.perf_counter()
numba_sum(arr)
nb_time = time.perf_counter() - start

print(f"Python: {py_time:.4f}s")
print(f"NumPy:  {np_time:.4f}s ({py_time/np_time:.1f}√ó faster)")
print(f"Numba:  {nb_time:.4f}s ({py_time/nb_time:.1f}√ó faster)")

# Typical Results:
# Python: 0.8500s
# NumPy:  0.0020s (425√ó faster)
# Numba:  0.0015s (567√ó faster)
```

**When to Use Numba:**

‚úÖ Numerical loops that NumPy can't vectorize
‚úÖ Custom algorithms with complex control flow
‚úÖ Functions called many times (amortizes compilation cost)
‚ùå Functions with Python objects (strings, dicts, etc.)
‚ùå Functions called only once (compilation overhead)

**Try This:** Optimize a custom algorithm with Numba:
```python
from numba import njit
import numpy as np

@njit
def custom_filter(arr, threshold):
    """Custom filtering algorithm (hard to vectorize)."""
    result = []
    for i in range(len(arr)):
        if arr[i] > threshold:
            result.append(arr[i] * 2)
        elif arr[i] < -threshold:
            result.append(arr[i] * -2)
    return np.array(result)

# Much faster than pure Python version
arr = np.random.randn(1_000_000)
filtered = custom_filter(arr, 0.5)
```

12.9.3 Cython

Cython compiles Python-like code to C extensions, providing C-level performance.

**Basic Example:**

```cython
# math_ops.pyx
cpdef int add(int x, int y):
    return x + y

cpdef double fast_sum(double[:] arr):
    cdef double total = 0.0
    cdef int i
    for i in range(len(arr)):
        total += arr[i]
    return total
```

**Performance Comparison:**

```python
# Pure Python
def python_sum(arr):
    total = 0.0
    for x in arr:
        total += x
    return total

# Cython (compiled)
# After compilation: import math_ops
# result = math_ops.fast_sum(arr)

# Benchmark results (typical):
# Python:  0.8500s (1.0√ó baseline)
# NumPy:   0.0020s (425√ó faster)
# Cython:  0.0010s (850√ó faster)
```

**When to Use Cython:**

‚úÖ Need maximum performance
‚úÖ Complex algorithms that benefit from static typing
‚úÖ Existing C libraries to wrap
‚ùå Simple operations (NumPy is easier)
‚ùå Rapid prototyping (compilation overhead)

12.9.4 Rust Extensions (PyO3)

Rust extensions provide the best balance of performance, safety, and modern tooling.

**Basic Example:**

```rust
// src/lib.rs
use pyo3::prelude::*;

#[pyfunction]
fn fast_sum(py: Python, arr: &PyArray1<f64>) -> PyResult<f64> {
    let array = unsafe { arr.as_array() };
    Ok(array.sum())
}

#[pymodule]
fn my_ext(_py: Python, m: &PyModule) -> PyResult<()> {
    m.add_function(wrap_pyfunction!(fast_sum, m)?)?;
    Ok(())
}
```

**Performance Benchmarks:**

```python
# After building Rust extension: import my_ext
# result = my_ext.fast_sum(arr)

# Typical Results:
# Python:  0.8500s (1.0√ó baseline)
# NumPy:   0.0020s (425√ó faster)
# Rust:    0.0008s (1063√ó faster)
```

**Advantages of Rust:**

‚úÖ Memory safety without GC overhead
‚úÖ Excellent performance
‚úÖ Modern tooling (cargo, maturin)
‚úÖ Easy to maintain
‚úÖ Can call C libraries safely

**Try This:** Create a simple Rust extension:
```bash
# Install maturin
pip install maturin

# Create new project
maturin init --name my_ext

# Build and install
maturin develop
```

12.9.5 multiprocessing: Parallel CPU Work

Multiprocessing bypasses the GIL by using separate processes.

**Performance Comparison:**

```python
from multiprocessing import Pool
from concurrent.futures import ProcessPoolExecutor
import time

def cpu_task(n: int) -> int:
    """CPU-intensive task."""
    return sum(i * i for i in range(n))

# Single process
start = time.perf_counter()
results = [cpu_task(1_000_000) for _ in range(8)]
single_time = time.perf_counter() - start

# Multiprocessing (4 cores)
start = time.perf_counter()
with ProcessPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(cpu_task, [1_000_000] * 8))
multi_time = time.perf_counter() - start

print(f"Single process: {single_time:.2f}s")
print(f"4 processes:    {multi_time:.2f}s")
print(f"Speedup:        {single_time/multi_time:.2f}√ó")

# Typical Results (4-core CPU):
# Single process: 8.00s
# 4 processes:    2.50s (3.2√ó speedup, not 4√ó due to overhead)
```

**When to Use:**

‚úÖ CPU-bound tasks that benefit from parallelization
‚úÖ Tasks that can be split into independent chunks
‚úÖ Long-running computations (amortizes process startup cost)
‚ùå Small, quick tasks (overhead > benefit)
‚ùå Tasks requiring frequent communication (IPC overhead)

12.10 Python 3.13: Tiered LLVM JIT

Python 3.13 introduces:

baseline JIT

optimizing tier

20‚Äì50% faster for many workloads

Requires:

PYTHON_JIT=1 python script.py

12.11 Python 3.14+: Free-Threading Mode

The GIL can be disabled via:

--disable-gil


But:

not fully stable

slower for single-thread

faster for parallel workloads

requires thread-safe libraries

12.12 Lazy Evaluation Patterns
12.12.1 Generators
values = (x*x for x in range(10_000_000))

12.12.2 iterators

Use itertools:

itertools.islice(iterable, 0, 1000)

12.12.3 Lazy loading objects

Example:

class Lazy:
    @property
    def data(self):
        if not hasattr(self, "_data"):
            self._data = load_data()
        return self._data

12.13 Extreme Optimization Patterns
‚úî avoid attribute lookups in hot loops

Move:

append = list.append
for x in data:
    append(x)


Significant speedup.

‚úî avoid try/except inside hot loops

Move exception handling outside loop.

‚úî consider PyPy

Useful for:

pure Python loops

long-running computations

12.14 Mini Example ‚Äî Fast Numeric Pipeline
import numpy as np

def pipeline():
    x = np.random.rand(1_000_000)
    y = np.sin(x)
    z = (x + y) * 2
    return z.mean()

print(pipeline())

12.15 Macro Example ‚Äî Log Analyzer (Optimized)

Uses:

mmap

regex precompilation

batching

generators

memory profiling

import re, mmap
from pathlib import Path

pattern = re.compile(rb"\[(?P<ts>.*?)\] (?P<lvl>\w+): (?P<msg>.*)")

def read_large(path):
    with open(path, "rb") as f, mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
        for m in pattern.finditer(mm):
            yield m.group("ts"), m.group("lvl"), m.group("msg")

for ts, lvl, msg in read_large("logs.bin"):
    ...

12.16 Pitfalls & Warnings

‚ö† Python loops are slow
‚ö† dicts are expensive memory-wise
‚ö† exception-heavy code becomes slow
‚ö† premature optimization is harmful
‚ö† pickle may degrade performance & adds security risks
‚ö† GIL prevents parallel CPU-bound threads (‚â§3.12)
‚ö† free-threading is not a magic bullet

12.17 Summary & Takeaways

Profile before optimizing

Use NumPy / Numba / Rust for CPU-bound code

Use asyncio for IO-bound code

Use mmap, buffered IO, batching for file work

Use caching effectively

Understand Python objects and memory overhead

Use slots, dataclasses, tuples for low memory

Effective performance requires architecture + tooling

12.18 Next Chapter

Proceed to:

üëâ Chapter 13 ‚Äî Security
Including:

OWASP Top 10 for Python

secure coding patterns

secrets management

dependency scanning

secure serialization

input validation

rate limiting

API hardening

common vulnerabilities
